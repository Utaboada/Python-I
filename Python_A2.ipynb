{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d757fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16d9edfbb67061dee0e78d6c8bd4bda4",
     "grade": false,
     "grade_id": "cell-6931830012e6c23e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Relación de ejercicios 2\n",
    "\n",
    "### Lee detenidamente las siguientes instrucciones y borra `raise NotImplementedError` en todas las celdas\n",
    "\n",
    "En cada trozo de código debes responder a la pregunta formulada, asegurándote de que el resultado queda guardado en la(s) variable(s) indicadas, que por defecto vienen inicializadas a `None`. Puedes usar variables intermedias siempre que el resultado final del cálculo quede guardado exactamente en la variable que indica el enunciado (debes reemplazar `None` el código necesario, pero nunca cambiar el nombre de las variables ya creadas). \n",
    "\n",
    "Después de cada ejercicio verás varias líneas de código ya hechas. Ejecútalas todas (no modifiques su código) y te dirán si tu solución es correcta o no, sin darte pistas de cómo se resuelve (el código de los tests no guarda relación con el código de la solución). Si la solución es correcta, no se mostrará nada, pero si es incorrecta, verás un error indicando cuál es el test que tu solución no pasa. Además de esas pruebas, se realizarán algunas más (ocultas) a la hora de puntuar el ejercicio, pero si tu código pasa con éxito las líneas que ves, puedes estar bastante seguro de que tu solución es correcta. Asegúrate de que todas las celdas de auto-evaluación indican que tu solución es correcta antes de subir el ejercicio a la plataforma. En caso contrario, no dudes en preguntar las dudas a través del foro -siempre explicando la duda en palabras, sin pegar código fuente-.\n",
    "\n",
    "Una vez finalizada la actividad, guarda tu fichero, ciérralo con File -> Close and Halt, y después vuélvelo a abrir y ejecútalo completo pinchando en Cell -> Run All Cells, y asegúrate de que no se lanza ningún error. De esta manera comprobarás que no has olvidado nada y que es posible ejecutarlo completo desde 0 y sin errores. No se corregirá ningún fichero que tenga errores de sintaxis y no se pueda, como mínimo, ejecutar completamente. No pasa nada si alguna de las comprobaciones lanza errores por ser incorrecta, pero el código de cada ejercicio no puede tener errores de sintaxis y debe al menos poderse ejecutar.\n",
    "\n",
    "RECUERDA SUBIR CADA UNO DE LOS FICHEROS .ipynb TAL CUAL (sueltos), SIN COMPRIMIR Y SIN CAMBIARLES EL NOMBRE. Los ficheros subidos deben tener **exactamente el mismo nombre de fichero que tenían cuando los recibiste**. No subas ningún PDF ni ningún fichero ZIP ni nada similar. La plataforma ya los separa automáticamente en carpetas que traen el nombre y apellidos del alumno, por lo que NO es necesario que lo pongas en ninguna parte.\n",
    "\n",
    "### Las funciones NUNCA deben hacer print() ni mostrar nada por pantalla, sino devolver el resultado del cálculo utilizando return. Recuerda que imprimir por pantalla no es devolver un resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1400d15",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a88b2b8cabf46f9e38cae8360084196",
     "grade": false,
     "grade_id": "ej1_enunciado",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 1 (1.5 puntos).** Crear el siguiente DataFrame y almacenarlo en la variable `personas`, poniendo los nombres de personas como columna de índice.\n",
    "\n",
    "| name      | age | height|weight|sex|\n",
    "|-----------|:---:|:-----:|:----:|:-:|\n",
    "| Alex      | 25  |   177 |   57 | F |\n",
    "| Lilly     | 31  |   163 |   69 | F |\n",
    "| Mark      | 23  |   190 |   83 | M |\n",
    "| Oliver    | 52  |   179 |   75 | M |\n",
    "| Martha    | 76  |   163 |   70 | F |\n",
    "| Lucas     | 49  |   183 |   83 | M |\n",
    "| Caroline  | 26  |   164 |   53 | F |\n",
    "\n",
    "- A continuación, crear en la variable `surname` un objeto Series (no un DataFrame) con el siguiente contenido en el orden que se muestra y con su correspondiente índice, y posteriormente añadir dicha columna al data.frame `personas` por la **izquierda** , SIN modificar el contenido de `personas` sino guardando el resultado en una nueva variable `personas_ampliado` de tipo DataFrame. Nótese que el orden de los elementos de la serie no coincide con el de las filas del DataFrame anterior, pero como están identificados con su índice, se podrá añadir correctamente. PISTA: puedes usar la función `assign(nueva_col=valores)` de los DataFrames, que te devolverá un nuevoDF (consulta la documentación [aquí](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html)).\n",
    "\n",
    "|           | surname  |\n",
    "|-----------|:---------|\n",
    "| Lilly     | Potter   |\n",
    "| Caroline  | Butter   |\n",
    "| Mark      | Marquez  |\n",
    "| Alex      | Song     |\n",
    "| Martha    | Argerich |\n",
    "| Lucas     | Brown    |\n",
    "| Oliver    | Atom     |\n",
    "\n",
    "- Usando la función `.str.cat(otra_serie, separador)`, de un objeto serie, modificar el DataFrame `personas_ampliado` añadiendo una nueva columna `surname_age` por la derecha, que contenga la concatenación de la serie (columna) `surname` con la serie resultante de convertir la serie `age` a string con el método `astype(str)`. Utiliza `\"-\"` (guión medio) como separador. PISTA: invoca a `.str.cat` sobre la serie `surname` pasando `age` como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8824ba9b",
   "metadata": {
    "deletable": false,
    "echo": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "710cb9c785eb8caf2af477d718bf064b",
     "grade": false,
     "grade_id": "ej1_solucion",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "output": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "personas = {\n",
    "    'name': ['Alex', 'Lilly', 'Mark', 'Oliver', 'Martha','Lucas','Caroline'],\n",
    "    'age': [25, 31, 23, 52, 76,49,26],\n",
    "    'height': [177, 163, 190, 179, 163,183,164],\n",
    "    'wwight': [57, 69,83, 75, 70,83,53],\n",
    "    'sex': ['F', 'F', 'M', 'M', 'F','M','F']\n",
    "}\n",
    "personas = pd.DataFrame(personas)\n",
    "personas.set_index('name', inplace=True)\n",
    "\n",
    "\n",
    "surname = pd.Series(['Potter', 'Butter', 'Marquez', 'Song', 'Argerich','Brown','Atom'],index=['Lilly', 'Caroline', 'Mark', 'Alex', 'Martha','Lucas','Oliver'])\n",
    "personas_ampliado = pd.concat([pd.DataFrame(surname, index=personas.index, columns=['surname']),personas], axis=1)\n",
    "\n",
    "\n",
    "personas_ampliado['surname_age'] = surname.str.cat(personas['age'].astype(str), sep='-')\n",
    "#checks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6dff70e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c47dcb1ff29acf6177fe5bce99a331f",
     "grade": true,
     "grade_id": "ej1-test",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "assert(isinstance(personas, pd.core.frame.DataFrame) and isinstance(personas_ampliado, pd.core.frame.DataFrame) and\n",
    "       isinstance(surname, pd.core.frame.Series))\n",
    "\n",
    "assert(personas.shape == (7, 4) and personas.index.size == 7 and personas.index[0] == \"Alex\")\n",
    "assert(personas_ampliado.shape == (7, 6) and personas_ampliado.index.size == 7 and personas_ampliado.index[0] == \"Alex\")\n",
    "assert(personas_ampliado.dtypes[\"surname_age\"].name == \"object\")\n",
    "assert(personas_ampliado[\"surname_age\"].loc[\"Lucas\"] == \"Brown-49\")\n",
    "assert(surname.index.to_list() == [\"Lilly\", \"Caroline\", \"Mark\", \"Alex\", \"Martha\", \"Lucas\", \"Oliver\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0823a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e238be6ebec4e7b6baa0869a91239f3",
     "grade": false,
     "grade_id": "ej2_enunciado",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 2 (3 puntos).** Usando el dataset `state_x77.csv`, leerlo a un DataFrame en la variable `state_x77` usando como índice la columna \"State\" que contiene los nombres de los estados. (recuerda el argumento `index_col`). \n",
    "\n",
    "- Una vez hecho esto, crear una variable `state_df` como copia de `state_x77` para después modificarla reemplazando por `None` **todos los valores de la columna `Income` para aquellos estados cuyo nombre empieza por `M`**. El contenido de `state_x77` no debe modificarse. Para esto, crear primero una máscara booleana sobre las filas y guardarla en la variable `estados_m_mask` utilizando los nombres de fila extraídos de `state_x77`. \n",
    "  * PISTA: el índice del DF no es una columna y por tanto, tampoco es una serie, pero puedes convertirlo a serie utilizando la función `to_series()` aplicada al índice del DF.\n",
    "  * PISTA: consulta la documentación de la función `.str.startswith` de las series [aquí](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.startswith.html).\n",
    "\n",
    "- En el propio `state_df` añadir una fila nueva cuyo nombre de fila sea `Total` que contenga en cada columna la suma de los valores de todas las filas de esa columna, usando la función `sum` de los DataFrames (no de las series) con `axis=0` (consulta la ayuda [aquí](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html)). La fila debe crearse con ese nombre directamente, en vez de indicar el nombre después. Los valores nulos deben ignorarse para la suma (consulta los argumentos de `sum`).\n",
    "\n",
    "- A continuación, crear una **función** `aniade_porcentajes(df)` que reciba un DataFrame cualquiera, acerca del cual podemos asumir que todas sus columnas son numéricas y que va a existir una **fila** con nombre `Total`. La función debe **devolver un nuevo DataFrame** que tendrá el **doble de columnas** que el DF pasado como argumento: la primera mitad de ellas serán idénticas al original, y la segunda mitad (es decir, las añadidas por la derecha) serán la traducción de las originales a porcentajes, dividiendo cada valor de cada celda entre el total de esa columna que se encuentra en la fila `Total` y multiplicando por 100. Los nombres de las nuevas columnas deben ser iguales a los de las columnas originales, pero con la terminación `_porc` (puedes usar `f\"{variable}_porc\"` o el operador de concatenación `+`). La función **no debe modificar en ningún momento el DF pasado como argumento**.\n",
    "  * *La función no debe estar pensada específicamente para el DF `state_df` sino para cualquier DF en el que se cumplan las condiciones descritas*. PISTA: crea una nueva variable dentro de la función llamada `resultado`, que inicialmente sea igual al argumento `df`, y ve añadiéndole las nuevas columnas.\n",
    "\n",
    "- Como caso particular, invocar la función creada sobre el data.frame `state_df` y guardar el resultado en la variable `state_df_porc`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7de7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_x77 = pd.read_csv(\"C:\\\\Users\\\\uxiat\\\\Desktop\\\\MasterBD\\\\Python\\\\state_x77.csv\", index_col=\"State\") #original con state como indice\n",
    "state_x77 = pd.DataFrame(state_x77)\n",
    "state_df = state_x77.copy()\n",
    "estados_m_mask = state_df.index.to_series().str.startswith('M')\n",
    "state_df['Income'].mask(estados_m_mask, None, inplace=True)\n",
    "\n",
    "state_df.loc['Total']= state_df.sum(skipna=True)\n",
    "\n",
    "\n",
    "def aniade_porcentajes(df):\n",
    "    resultado = df.copy()\n",
    "    for col in resultado.columns:\n",
    "        resultado[col+'_porc'] = (resultado[col]/resultado.loc['Total'][col])*100\n",
    "    state_df_porc = pd.concat([resultado], axis=1)\n",
    "    return state_df_porc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b72d3a6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "015a857e8a3c9231dd8fdbe33c0bef17",
     "grade": true,
     "grade_id": "ej2-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(state_df, pd.core.frame.DataFrame) and state_df.shape == (51, 8))\n",
    "assert(state_df.index.to_list() == state_x77.index.to_list() + [\"Total\"])\n",
    "\n",
    "assert(isinstance(estados_m_mask, pd.core.frame.Series) and estados_m_mask.dtypes.name == \"bool\" \n",
    "       and estados_m_mask.size == 50 and estados_m_mask.sum() == 8)\n",
    "\n",
    "result_df = aniade_porcentajes(state_df)\n",
    "assert(result_df.shape == (51, 16))\n",
    "assert(len([x for x in list(map(lambda x: x + \"_porc\", state_df.columns)) if x in list(result_df.columns)]) == 8) # comprobar nuevos nombres\n",
    "assert(round(aniade_porcentajes(state_df).loc[\"Virginia\", \"Income_porc\"], 3) == 2.515)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dae476",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b358eeccf049ef58184ee3fed2bd6d0",
     "grade": false,
     "grade_id": "ej3-enunciado",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 3 (2 puntos)** Con la variable `state_df` definida anteriormente:\n",
    "\n",
    "- **Copiar** su contenido a una nueva variable `state_df_abb` (no es suficiente con sólo asignarla). \n",
    "- Añadir a `state_df_abb` una nueva columna `Division` (con D mayúscula) que sea de tipo **Series** y cuyo contenido sea el contenido de la variable `division` que puedes encontrar creada a continuación y que no debes modificar. El orden de los valores corresponde a los estados en el mismo que en el DF `state_df`. Para añadirla como series, primero crea un objeto Series cuyo índice sea exactamente el mismo que el del DF `state_df_abb`, almacénalo en la variable `division_series`, y finalmente añade dicho objeto como columna.\n",
    "\n",
    "- Utilizando la función `groupby` con la variable `Division` como agrupadora y la variable numérica `Income` (ingresos) como objetivo, calcular el **ingreso medio en cada zona de EEUU** y almacenar el resultado en la variable `ingresos_medios`. Dentro de `agg` puedes usar la función `np.mean` de Numpy (recuerda que habíamos introducido en el ejercicio anterior varios `None` justamente en esa columna). Asegúrate de que la división quede *como el índice* del DF devuelto como resultado, que tendrá una sola columna como tal además del índice.\n",
    "\n",
    "- Añadir a `state_df_abb` una nueva columna `Division_mean_income` que tenga, para cada estado, el ingreso medio de la división a la que pertenece el estado. PISTA: utiliza `.loc[]` sobre la Serie (columna) `Income` del DF `ingresos_medios`, aprovechando que su índice son los nombres de división, y por tanto, puedes indexar sobre dicha serie si pasas los nombres de los elementos de la serie en el orden deseado. Dicho orden viene dado por la columna `Division` del DF `state_df_abb` (está permitido pasar una Serie como argumento a `loc`). **No está permitido usar join ni merge ni nada similar**.\n",
    "  * PISTA: convierte la serie devuelta por `loc` en una lista de números reales con `to_list()` antes de asignarla al DF. De lo contrario, obtendrás todo el rato valores NaN ya que la serie tiene como índice el nombre de la división, lo cual no coincide con el índice del DF al que quieres asignar la columna, que son los nombres de estados. Al convertir a lista evitamos que Python intente hacer coincidir los índices.\n",
    "\n",
    "- Seleccionar las filas de `state_df_abb` correspondientes a estados con un ingreso (columna `Income`) mayor que el ingreso medio de esa división (columna `Division_mean_income`). Para dichas filas seleccionar solamente las columnas `Population`, `Income`, `Division_mean_income` y `Area`. Guardar el resultado en la variable `state_df_high_income`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5181f462",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47d16a238c46b824f29617d0d09a1b6b",
     "grade": false,
     "grade_id": "ej3-respuesta",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Division</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>East North Central</th>\n",
       "      <td>4648.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>East South Central</th>\n",
       "      <td>3719.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle Atlantic</th>\n",
       "      <td>4863.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mountain</th>\n",
       "      <td>4410.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New England</th>\n",
       "      <td>4523.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ninguna</th>\n",
       "      <td>186917.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pacific</th>\n",
       "      <td>5183.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Atlantic</th>\n",
       "      <td>4220.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West North Central</th>\n",
       "      <td>4611.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West South Central</th>\n",
       "      <td>3773.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Income\n",
       "Division                         \n",
       "East North Central    4648.500000\n",
       "East South Central    3719.000000\n",
       "Middle Atlantic       4863.000000\n",
       "Mountain              4410.142857\n",
       "New England           4523.500000\n",
       "Ninguna             186917.000000\n",
       "Pacific               5183.200000\n",
       "South Atlantic        4220.428571\n",
       "West North Central    4611.800000\n",
       "West South Central    3773.500000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "division = [\n",
    "    \"East South Central\", \"Pacific\", \"Mountain\", \"West South Central\", \"Pacific\", \"Mountain\", \"New England\", \"South Atlantic\", \n",
    "    \"South Atlantic\", \"South Atlantic\", \"Pacific\", \"Mountain\", \"East North Central\", \"East North Central\", \n",
    "    \"West North Central\", \"West North Central\", \"East South Central\", \"West South Central\", \"New England\", \"South Atlantic\",\n",
    "    \"New England\", \"East North Central\", \"West North Central\", \"East South Central\", \"West North Central\", \"Mountain\",\n",
    "    \"West North Central\", \"Mountain\", \"New England\", \"Middle Atlantic\", \"Mountain\", \"Middle Atlantic\", \"South Atlantic\",\n",
    "    \"West North Central\", \"East North Central\", \"West South Central\", \"Pacific\", \"Middle Atlantic\", \"New England\",\n",
    "    \"South Atlantic\", \"West North Central\", \"East South Central\", \"West South Central\", \"Mountain\", \"New England\",\n",
    "    \"South Atlantic\", \"Pacific\", \"South Atlantic\", \"East North Central\", \"Mountain\", \"Ninguna\"]\n",
    "    \n",
    "state_df_abb = state_df.copy()\n",
    "division_series = pd.Series(division, index=state_df_abb.index)\n",
    "state_df_abb[\"Division\"] = division_series\n",
    "ingresos_medios = pd.DataFrame(state_df_abb.groupby(\"Division\")[\"Income\"].mean())\n",
    "\n",
    "division_mean_income = ingresos_medios.loc[state_df_abb[\"Division\"], \"Income\"].to_list()\n",
    "state_df_abb[\"Division_mean_income\"] = division_mean_income\n",
    "\n",
    "state_df_high_income = pd.DataFrame(state_df_abb[state_df_abb[\"Income\"] > state_df_abb[\"Division_mean_income\"]],\n",
    "    columns=[\"Population\",\"Income\", \"Division_mean_income\",\"Area\"])\n",
    "\n",
    "ingresos_medios\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf132189",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe50d523945e18d796a2f2468b132573",
     "grade": true,
     "grade_id": "ej3-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(state_df_abb, pd.core.frame.DataFrame) and\n",
    "       isinstance(state_df_high_income, pd.core.frame.DataFrame))\n",
    "\n",
    "assert(state_df_abb.shape == (51, 10))\n",
    "assert(state_df_high_income.shape == (19, 4))\n",
    "\n",
    "assert((division_series.index == state_df_abb.index).all())\n",
    "assert(isinstance(state_df_abb[\"Division\"], pd.core.series.Series))    # Division no debe estar como strings sino como serie\n",
    "\n",
    "assert(round(ingresos_medios.Income.median(), 2) == 4567.65)\n",
    "assert(state_df_abb.Division_mean_income.median() == 4523.5)\n",
    "\n",
    "assert(round(state_df_high_income.Area.mean(), 2) ==  93832.37 and\n",
    "       state_df_high_income.Population.median() == 2715)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b59d17",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f4389a9602e12e4546b59e718ada324",
     "grade": false,
     "grade_id": "ej4-enunciado",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 4 (1.5 puntos)** Crear una función `aniade_proporciones` que reciba un solo argumento de tipo DataFrame, busque todas las columnas cuyo nombre termine en `_porc`, y añada por el final tantas columnas como haya encontrado, pero cambiando `_porc` por `_prop` que contendrá esa misma cantidad pero en tanto por uno (es decir, proporción), lo cual se obtiene dividiendo por 100.0 cada columna de porcentajes. El DF que la función recibe como argumento **no debe modificarse**.\n",
    "* PISTA: puedes hacerlo sin bucles, primero encontrando el sub-dataframe formado por solamente esas columnas (para lo cual necesitarás crear, mediante listas por comprensión, una lista que contenga solamente esas columnas, y luego usarla con `.loc`), modificando todos sus elementos dividiendo el DF por 100 (puedes dividir un DF entre un número directamente) y por último renombrar sus columnas, para finalmente añadirlo al existente por la derecha, **sin usar merge ni join** sino como operación de concatenación, puesto que ya sabemos que ambos DF tienen las filas en el mismo orden (consulta la documentación de [la función pd.concat con axis=1](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)). En este caso el primer argumento de pd.concat debe ser una *lista* de los dos DF que queremos concatenar.\n",
    "* PISTA: \n",
    "* La función debe estar pensada para cualquier DF que tenga columnas numéricas cuyo nombre termine en _porc\n",
    "* Como caso particular, invocar a la función sobre el DF `state_df_porc` creado en el ejercicio 2 y almacena el resultado en la variable `states_porc_prop_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "147dd3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aniade_proporciones(df):\n",
    "    cols_porc = [col for col in df.columns if col.endswith('_porc')]\n",
    "    df_porc = df.loc[:, cols_porc]\n",
    "    df_prop = df_porc.div(100.0)\n",
    "    df_prop.columns = [col.replace('_porc', '_prop') for col in df_prop.columns]\n",
    "    return pd.concat([df, df_prop], axis=1)\n",
    "state_df_porc =aniade_porcentajes(state_df)\n",
    "states_porc_prop_df = aniade_proporciones(state_df_porc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cfc60ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e49cbcfe389200702c63520c9cf20331",
     "grade": true,
     "grade_id": "ej4-test",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "aniade_proporciones_source = inspect.getsource(aniade_proporciones)\n",
    "assert(\"state_df_porc\" not in aniade_proporciones_source and       # No se puede referenciar ningún DF externo a la función\n",
    "       \"state_x77\" not in aniade_proporciones_source)\n",
    "mi_resultado = aniade_proporciones(state_df_porc)\n",
    "assert(isinstance(mi_resultado, pd.core.frame.DataFrame))\n",
    "assert(len(state_df_porc.columns) == 16)                  # El DF original no debe haberse modificado\n",
    "assert(len(mi_resultado.columns) == 24)\n",
    "assert(set(['Area_porc', 'Population_prop', 'Income_prop', 'Illiteracy_prop',\n",
    "       'Life Exp_prop', 'Murder_prop', 'HS Grad_prop', 'Frost_prop', 'Area_prop']).issubset(mi_resultado.columns))\n",
    "assert(round(mi_resultado[\"Area_prop\"].median(), 5) == 0.0154)\n",
    "assert(round(mi_resultado[\"Frost_prop\"].median(), 5) == 0.02202)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2ee4e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "579aeb124991dfa2b58dfccac8c5117f",
     "grade": false,
     "grade_id": "ej5-enunciado",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 5 (2 puntos).** Leer el fichero `weather_usa.csv` en la variable `weather_df` y unirlo con el DF contenido en `state_x77` de la manera adecuada. Recuerda que los nombres de estados están en el *índice* de `state_x77`, no son ninguna columna. Almacenar el resultado de la operación en la variable `x77_weather_df`.\n",
    "\n",
    "A continuación:\n",
    "\n",
    "* Guardar en la variable `life_sun_corr` el coeficiente de correlación lineal entre las horas de sol de la capital de un estado y la esperanza de vida de ese estado. Aplícalo a dos objetos Series (las dos columnas) con la función `pd.Series.corr()`.\n",
    "* Crear en la variable `north_south_regex` una expresión regular para identificar los estados que contengan la palabra \"North\" o \"South\" seguida de exactamente un separador, que no tiene por qué ser \" \" sino que puede ser cualquier carácter separador. Utiliza la sintaxis `f\"expr\"`.\n",
    "* Aplicar la expresión regular al índice del DataFrame `x77_weather` y guardar el resultado en `north_south_mask`. PISTA: consulta la documentación de la función [Series.str.match](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.match.html) de los objetos Series. Necesitarás convertir el índice del DF en un objeto Series con el método `.to_series()` aplicado al índice.\n",
    "* Aplica a `x77_weather_df` la máscara construida para obtener un DF que sólo contenga dichas filas, y guárdalo en la variable `north_south_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f99a4106",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1012d35af096beb25e53ee731ad30933",
     "grade": false,
     "grade_id": "ej5-respuesta",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (994022197.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [30], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    weather_df = pd.read_csv(\"C:\\Users\\uxiat\\Desktop\\MasterBD\\Python\\weather_usa\")\u001b[0m\n\u001b[1;37m                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "weather_df = pd.read_csv(\"C:\\Users\\uxiat\\Desktop\\MasterBD\\Python\\weather_usa\")\n",
    "x77_weather_df = None\n",
    "north_south_regex = None\n",
    "north_south_mask = None\n",
    "north_south_df = None\n",
    "life_sun_corr = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "607b1586",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [64], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m life_sun_corr \u001b[39m=\u001b[39m x77_weather_df[\u001b[39m'\u001b[39m\u001b[39mLife Exp\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcorr(x77_weather_df[\u001b[39m'\u001b[39m\u001b[39msun_hours\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m north_south_regex \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(North|South).*\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m north_south_mask \u001b[39m=\u001b[39m x77_weather_df\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mto_series()\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39mmatch(north_south_regex)\n\u001b[0;32m      6\u001b[0m north_south_df \u001b[39m=\u001b[39m x77_weather_df[north_south_mask]\n\u001b[0;32m      8\u001b[0m \u001b[39mtype\u001b[39m(x77_weather_df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mto_series())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:5907\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5900\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5901\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5902\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5903\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5904\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5905\u001b[0m ):\n\u001b[0;32m   5906\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5907\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\accessor.py:183\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[1;32m--> 183\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[0;32m    184\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\strings\\accessor.py:182\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_\u001b[39;00m \u001b[39mimport\u001b[39;00m StringDtype\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(data)\n\u001b[0;32m    183\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_categorical \u001b[39m=\u001b[39m is_categorical_dtype(data\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    184\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_string \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(data\u001b[39m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\strings\\accessor.py:236\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    233\u001b[0m inferred_dtype \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39minfer_dtype(values, skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m inferred_dtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_types:\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .str accessor with string values!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "weather_df = pd.read_csv(\"C:\\\\Users\\\\uxiat\\\\Desktop\\\\MasterBD\\\\Python\\\\weather_usa.csv\")\n",
    "x77_weather_df = state_x77.merge(weather_df, left_index=True, right_on=\"State\")\n",
    "life_sun_corr = x77_weather_df['Life Exp'].corr(x77_weather_df['sun_hours'])\n",
    "north_south_regex = f\"(North|South).*\"\n",
    "north_south_mask = x77_weather_df.index.to_series().str.match(north_south_regex)\n",
    "north_south_df = x77_weather_df[north_south_mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ebc93daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Income</th>\n",
       "      <th>Illiteracy</th>\n",
       "      <th>Life Exp</th>\n",
       "      <th>Murder</th>\n",
       "      <th>HS Grad</th>\n",
       "      <th>Frost</th>\n",
       "      <th>Area</th>\n",
       "      <th>rain_mm</th>\n",
       "      <th>temp_celsius</th>\n",
       "      <th>sun_city</th>\n",
       "      <th>sun_perc</th>\n",
       "      <th>sun_hours</th>\n",
       "      <th>clear_days</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>3615</td>\n",
       "      <td>3624</td>\n",
       "      <td>2.1</td>\n",
       "      <td>69.05</td>\n",
       "      <td>15.1</td>\n",
       "      <td>41.3</td>\n",
       "      <td>20</td>\n",
       "      <td>50708</td>\n",
       "      <td>1480</td>\n",
       "      <td>17.1</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2641.0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>365</td>\n",
       "      <td>6315</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.31</td>\n",
       "      <td>11.3</td>\n",
       "      <td>66.7</td>\n",
       "      <td>152</td>\n",
       "      <td>566432</td>\n",
       "      <td>572</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2061.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>2212</td>\n",
       "      <td>4530</td>\n",
       "      <td>1.8</td>\n",
       "      <td>70.55</td>\n",
       "      <td>7.8</td>\n",
       "      <td>58.1</td>\n",
       "      <td>15</td>\n",
       "      <td>113417</td>\n",
       "      <td>345</td>\n",
       "      <td>15.7</td>\n",
       "      <td>Tucson</td>\n",
       "      <td>85.0</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>2110</td>\n",
       "      <td>3378</td>\n",
       "      <td>1.9</td>\n",
       "      <td>70.66</td>\n",
       "      <td>10.1</td>\n",
       "      <td>39.9</td>\n",
       "      <td>65</td>\n",
       "      <td>51945</td>\n",
       "      <td>1284</td>\n",
       "      <td>15.8</td>\n",
       "      <td>Fort Smith</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2771.0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>21198</td>\n",
       "      <td>5114</td>\n",
       "      <td>1.1</td>\n",
       "      <td>71.71</td>\n",
       "      <td>10.3</td>\n",
       "      <td>62.6</td>\n",
       "      <td>20</td>\n",
       "      <td>156361</td>\n",
       "      <td>563</td>\n",
       "      <td>15.2</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3055.0</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>2541</td>\n",
       "      <td>4884</td>\n",
       "      <td>0.7</td>\n",
       "      <td>72.06</td>\n",
       "      <td>6.8</td>\n",
       "      <td>63.9</td>\n",
       "      <td>166</td>\n",
       "      <td>103766</td>\n",
       "      <td>405</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Grand Junction</td>\n",
       "      <td>71.0</td>\n",
       "      <td>3204.0</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Connecticut</th>\n",
       "      <td>3100</td>\n",
       "      <td>5348</td>\n",
       "      <td>1.1</td>\n",
       "      <td>72.48</td>\n",
       "      <td>3.1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>139</td>\n",
       "      <td>4862</td>\n",
       "      <td>1279</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Hartford</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2585.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delaware</th>\n",
       "      <td>579</td>\n",
       "      <td>4809</td>\n",
       "      <td>0.9</td>\n",
       "      <td>70.06</td>\n",
       "      <td>6.2</td>\n",
       "      <td>54.6</td>\n",
       "      <td>103</td>\n",
       "      <td>1982</td>\n",
       "      <td>1160</td>\n",
       "      <td>12.9</td>\n",
       "      <td>Wilmington</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>8277</td>\n",
       "      <td>4815</td>\n",
       "      <td>1.3</td>\n",
       "      <td>70.66</td>\n",
       "      <td>10.7</td>\n",
       "      <td>52.6</td>\n",
       "      <td>11</td>\n",
       "      <td>54090</td>\n",
       "      <td>1385</td>\n",
       "      <td>21.5</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2927.0</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Georgia</th>\n",
       "      <td>4931</td>\n",
       "      <td>4091</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.54</td>\n",
       "      <td>13.9</td>\n",
       "      <td>40.6</td>\n",
       "      <td>60</td>\n",
       "      <td>58073</td>\n",
       "      <td>1287</td>\n",
       "      <td>17.5</td>\n",
       "      <td>Macon</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2986.0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawaii</th>\n",
       "      <td>868</td>\n",
       "      <td>4963</td>\n",
       "      <td>1.9</td>\n",
       "      <td>73.60</td>\n",
       "      <td>6.2</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0</td>\n",
       "      <td>6425</td>\n",
       "      <td>1618</td>\n",
       "      <td>21.1</td>\n",
       "      <td>Honolulu</td>\n",
       "      <td>71.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idaho</th>\n",
       "      <td>813</td>\n",
       "      <td>4119</td>\n",
       "      <td>0.6</td>\n",
       "      <td>71.87</td>\n",
       "      <td>5.3</td>\n",
       "      <td>59.5</td>\n",
       "      <td>126</td>\n",
       "      <td>82677</td>\n",
       "      <td>481</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Boise</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2993.0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>11197</td>\n",
       "      <td>5107</td>\n",
       "      <td>0.9</td>\n",
       "      <td>70.14</td>\n",
       "      <td>10.3</td>\n",
       "      <td>52.6</td>\n",
       "      <td>127</td>\n",
       "      <td>55748</td>\n",
       "      <td>996</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Peoria</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2567.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana</th>\n",
       "      <td>5313</td>\n",
       "      <td>4458</td>\n",
       "      <td>0.7</td>\n",
       "      <td>70.88</td>\n",
       "      <td>7.1</td>\n",
       "      <td>52.9</td>\n",
       "      <td>122</td>\n",
       "      <td>36097</td>\n",
       "      <td>1060</td>\n",
       "      <td>10.9</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2440.0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>2861</td>\n",
       "      <td>4628</td>\n",
       "      <td>0.5</td>\n",
       "      <td>72.56</td>\n",
       "      <td>2.3</td>\n",
       "      <td>59.0</td>\n",
       "      <td>140</td>\n",
       "      <td>55941</td>\n",
       "      <td>864</td>\n",
       "      <td>8.8</td>\n",
       "      <td>Des Moines</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2691.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>2280</td>\n",
       "      <td>4669</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72.58</td>\n",
       "      <td>4.5</td>\n",
       "      <td>59.9</td>\n",
       "      <td>114</td>\n",
       "      <td>81787</td>\n",
       "      <td>733</td>\n",
       "      <td>12.4</td>\n",
       "      <td>Wichita</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2922.0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kentucky</th>\n",
       "      <td>3387</td>\n",
       "      <td>3712</td>\n",
       "      <td>1.6</td>\n",
       "      <td>70.10</td>\n",
       "      <td>10.6</td>\n",
       "      <td>38.5</td>\n",
       "      <td>95</td>\n",
       "      <td>39650</td>\n",
       "      <td>1242</td>\n",
       "      <td>13.1</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2514.0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Louisiana</th>\n",
       "      <td>3806</td>\n",
       "      <td>3545</td>\n",
       "      <td>2.8</td>\n",
       "      <td>68.76</td>\n",
       "      <td>13.2</td>\n",
       "      <td>42.2</td>\n",
       "      <td>12</td>\n",
       "      <td>44930</td>\n",
       "      <td>1528</td>\n",
       "      <td>19.1</td>\n",
       "      <td>New Orleans</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>1058</td>\n",
       "      <td>3694</td>\n",
       "      <td>0.7</td>\n",
       "      <td>70.39</td>\n",
       "      <td>2.7</td>\n",
       "      <td>54.7</td>\n",
       "      <td>161</td>\n",
       "      <td>30920</td>\n",
       "      <td>1072</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Portland</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2513.0</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryland</th>\n",
       "      <td>4122</td>\n",
       "      <td>5299</td>\n",
       "      <td>0.9</td>\n",
       "      <td>70.22</td>\n",
       "      <td>8.5</td>\n",
       "      <td>52.3</td>\n",
       "      <td>101</td>\n",
       "      <td>9891</td>\n",
       "      <td>1131</td>\n",
       "      <td>12.3</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2582.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>5814</td>\n",
       "      <td>4755</td>\n",
       "      <td>1.1</td>\n",
       "      <td>71.83</td>\n",
       "      <td>3.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>103</td>\n",
       "      <td>7826</td>\n",
       "      <td>1211</td>\n",
       "      <td>8.8</td>\n",
       "      <td>Boston</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2634.0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>9111</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.9</td>\n",
       "      <td>70.63</td>\n",
       "      <td>11.1</td>\n",
       "      <td>52.8</td>\n",
       "      <td>125</td>\n",
       "      <td>56817</td>\n",
       "      <td>833</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Lansing</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2392.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>3921</td>\n",
       "      <td>4675</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72.96</td>\n",
       "      <td>2.3</td>\n",
       "      <td>57.6</td>\n",
       "      <td>160</td>\n",
       "      <td>79289</td>\n",
       "      <td>693</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2711.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>2341</td>\n",
       "      <td>3098</td>\n",
       "      <td>2.4</td>\n",
       "      <td>68.09</td>\n",
       "      <td>12.5</td>\n",
       "      <td>41.0</td>\n",
       "      <td>50</td>\n",
       "      <td>47296</td>\n",
       "      <td>1499</td>\n",
       "      <td>17.4</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>4767</td>\n",
       "      <td>4254</td>\n",
       "      <td>0.8</td>\n",
       "      <td>70.69</td>\n",
       "      <td>9.3</td>\n",
       "      <td>48.8</td>\n",
       "      <td>108</td>\n",
       "      <td>68995</td>\n",
       "      <td>1071</td>\n",
       "      <td>12.5</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2690.0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>746</td>\n",
       "      <td>4347</td>\n",
       "      <td>0.6</td>\n",
       "      <td>70.56</td>\n",
       "      <td>5.0</td>\n",
       "      <td>59.2</td>\n",
       "      <td>155</td>\n",
       "      <td>145587</td>\n",
       "      <td>390</td>\n",
       "      <td>5.9</td>\n",
       "      <td>Helena</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2698.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <td>1544</td>\n",
       "      <td>4508</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72.60</td>\n",
       "      <td>2.9</td>\n",
       "      <td>59.3</td>\n",
       "      <td>139</td>\n",
       "      <td>76483</td>\n",
       "      <td>599</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2762.0</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>590</td>\n",
       "      <td>5149</td>\n",
       "      <td>0.5</td>\n",
       "      <td>69.03</td>\n",
       "      <td>11.5</td>\n",
       "      <td>65.2</td>\n",
       "      <td>188</td>\n",
       "      <td>109889</td>\n",
       "      <td>241</td>\n",
       "      <td>9.9</td>\n",
       "      <td>Reno</td>\n",
       "      <td>79.0</td>\n",
       "      <td>3646.0</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>812</td>\n",
       "      <td>4281</td>\n",
       "      <td>0.7</td>\n",
       "      <td>71.23</td>\n",
       "      <td>3.3</td>\n",
       "      <td>57.6</td>\n",
       "      <td>174</td>\n",
       "      <td>9027</td>\n",
       "      <td>1103</td>\n",
       "      <td>6.6</td>\n",
       "      <td>Concord</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2519.0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>7333</td>\n",
       "      <td>5237</td>\n",
       "      <td>1.1</td>\n",
       "      <td>70.93</td>\n",
       "      <td>5.2</td>\n",
       "      <td>52.5</td>\n",
       "      <td>115</td>\n",
       "      <td>7521</td>\n",
       "      <td>1196</td>\n",
       "      <td>11.5</td>\n",
       "      <td>Atlantic City</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>1144</td>\n",
       "      <td>3601</td>\n",
       "      <td>2.2</td>\n",
       "      <td>70.32</td>\n",
       "      <td>9.7</td>\n",
       "      <td>55.2</td>\n",
       "      <td>120</td>\n",
       "      <td>121412</td>\n",
       "      <td>370</td>\n",
       "      <td>11.9</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3415.0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>18076</td>\n",
       "      <td>4903</td>\n",
       "      <td>1.4</td>\n",
       "      <td>70.55</td>\n",
       "      <td>10.9</td>\n",
       "      <td>52.7</td>\n",
       "      <td>82</td>\n",
       "      <td>47831</td>\n",
       "      <td>1062</td>\n",
       "      <td>7.4</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina</th>\n",
       "      <td>5441</td>\n",
       "      <td>3875</td>\n",
       "      <td>1.8</td>\n",
       "      <td>69.21</td>\n",
       "      <td>11.1</td>\n",
       "      <td>38.5</td>\n",
       "      <td>80</td>\n",
       "      <td>48798</td>\n",
       "      <td>1279</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Greensboro</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2651.0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>637</td>\n",
       "      <td>5087</td>\n",
       "      <td>0.8</td>\n",
       "      <td>72.78</td>\n",
       "      <td>1.4</td>\n",
       "      <td>50.3</td>\n",
       "      <td>186</td>\n",
       "      <td>69273</td>\n",
       "      <td>452</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Bismarck</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2738.0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>10735</td>\n",
       "      <td>4561</td>\n",
       "      <td>0.8</td>\n",
       "      <td>70.82</td>\n",
       "      <td>7.4</td>\n",
       "      <td>53.2</td>\n",
       "      <td>124</td>\n",
       "      <td>40975</td>\n",
       "      <td>993</td>\n",
       "      <td>10.4</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2183.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>2715</td>\n",
       "      <td>3983</td>\n",
       "      <td>1.1</td>\n",
       "      <td>71.42</td>\n",
       "      <td>6.4</td>\n",
       "      <td>51.6</td>\n",
       "      <td>82</td>\n",
       "      <td>68782</td>\n",
       "      <td>927</td>\n",
       "      <td>15.3</td>\n",
       "      <td>Oklahoma City</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3089.0</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>2284</td>\n",
       "      <td>4660</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72.13</td>\n",
       "      <td>4.2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>44</td>\n",
       "      <td>96184</td>\n",
       "      <td>695</td>\n",
       "      <td>9.1</td>\n",
       "      <td>Portland</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>11860</td>\n",
       "      <td>4449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.43</td>\n",
       "      <td>6.1</td>\n",
       "      <td>50.2</td>\n",
       "      <td>126</td>\n",
       "      <td>44966</td>\n",
       "      <td>1089</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Harrisburg</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2614.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>931</td>\n",
       "      <td>4558</td>\n",
       "      <td>1.3</td>\n",
       "      <td>71.90</td>\n",
       "      <td>2.4</td>\n",
       "      <td>46.4</td>\n",
       "      <td>127</td>\n",
       "      <td>1049</td>\n",
       "      <td>1218</td>\n",
       "      <td>10.1</td>\n",
       "      <td>Providence</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Carolina</th>\n",
       "      <td>2816</td>\n",
       "      <td>3635</td>\n",
       "      <td>2.3</td>\n",
       "      <td>67.96</td>\n",
       "      <td>11.6</td>\n",
       "      <td>37.8</td>\n",
       "      <td>65</td>\n",
       "      <td>30225</td>\n",
       "      <td>1264</td>\n",
       "      <td>16.9</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2826.0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>681</td>\n",
       "      <td>4167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>72.08</td>\n",
       "      <td>1.7</td>\n",
       "      <td>53.3</td>\n",
       "      <td>172</td>\n",
       "      <td>75955</td>\n",
       "      <td>511</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Huron</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2947.0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tennessee</th>\n",
       "      <td>4173</td>\n",
       "      <td>3821</td>\n",
       "      <td>1.7</td>\n",
       "      <td>70.11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>70</td>\n",
       "      <td>41328</td>\n",
       "      <td>1376</td>\n",
       "      <td>14.2</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2510.0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>12237</td>\n",
       "      <td>4188</td>\n",
       "      <td>2.2</td>\n",
       "      <td>70.90</td>\n",
       "      <td>12.2</td>\n",
       "      <td>47.4</td>\n",
       "      <td>35</td>\n",
       "      <td>262134</td>\n",
       "      <td>734</td>\n",
       "      <td>18.2</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>1203</td>\n",
       "      <td>4022</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72.90</td>\n",
       "      <td>4.5</td>\n",
       "      <td>67.3</td>\n",
       "      <td>137</td>\n",
       "      <td>82096</td>\n",
       "      <td>310</td>\n",
       "      <td>9.2</td>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3029.0</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>472</td>\n",
       "      <td>3907</td>\n",
       "      <td>0.6</td>\n",
       "      <td>71.64</td>\n",
       "      <td>5.5</td>\n",
       "      <td>57.1</td>\n",
       "      <td>168</td>\n",
       "      <td>9267</td>\n",
       "      <td>1085</td>\n",
       "      <td>6.1</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>4981</td>\n",
       "      <td>4701</td>\n",
       "      <td>1.4</td>\n",
       "      <td>70.08</td>\n",
       "      <td>9.5</td>\n",
       "      <td>47.8</td>\n",
       "      <td>85</td>\n",
       "      <td>39780</td>\n",
       "      <td>1125</td>\n",
       "      <td>12.8</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2829.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>3559</td>\n",
       "      <td>4864</td>\n",
       "      <td>0.6</td>\n",
       "      <td>71.72</td>\n",
       "      <td>4.3</td>\n",
       "      <td>63.5</td>\n",
       "      <td>32</td>\n",
       "      <td>66570</td>\n",
       "      <td>976</td>\n",
       "      <td>9.1</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>1799</td>\n",
       "      <td>3617</td>\n",
       "      <td>1.4</td>\n",
       "      <td>69.48</td>\n",
       "      <td>6.7</td>\n",
       "      <td>41.6</td>\n",
       "      <td>100</td>\n",
       "      <td>24070</td>\n",
       "      <td>1147</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Beckley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>4589</td>\n",
       "      <td>4468</td>\n",
       "      <td>0.7</td>\n",
       "      <td>72.48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>149</td>\n",
       "      <td>54464</td>\n",
       "      <td>829</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Madison</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2428.0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>376</td>\n",
       "      <td>4566</td>\n",
       "      <td>0.6</td>\n",
       "      <td>70.29</td>\n",
       "      <td>6.9</td>\n",
       "      <td>62.9</td>\n",
       "      <td>173</td>\n",
       "      <td>97203</td>\n",
       "      <td>328</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Lander</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3073.0</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Population  Income  Illiteracy  Life Exp  Murder  HS Grad  \\\n",
       "State                                                                       \n",
       "Alabama               3615    3624         2.1     69.05    15.1     41.3   \n",
       "Alaska                 365    6315         1.5     69.31    11.3     66.7   \n",
       "Arizona               2212    4530         1.8     70.55     7.8     58.1   \n",
       "Arkansas              2110    3378         1.9     70.66    10.1     39.9   \n",
       "California           21198    5114         1.1     71.71    10.3     62.6   \n",
       "Colorado              2541    4884         0.7     72.06     6.8     63.9   \n",
       "Connecticut           3100    5348         1.1     72.48     3.1     56.0   \n",
       "Delaware               579    4809         0.9     70.06     6.2     54.6   \n",
       "Florida               8277    4815         1.3     70.66    10.7     52.6   \n",
       "Georgia               4931    4091         2.0     68.54    13.9     40.6   \n",
       "Hawaii                 868    4963         1.9     73.60     6.2     61.9   \n",
       "Idaho                  813    4119         0.6     71.87     5.3     59.5   \n",
       "Illinois             11197    5107         0.9     70.14    10.3     52.6   \n",
       "Indiana               5313    4458         0.7     70.88     7.1     52.9   \n",
       "Iowa                  2861    4628         0.5     72.56     2.3     59.0   \n",
       "Kansas                2280    4669         0.6     72.58     4.5     59.9   \n",
       "Kentucky              3387    3712         1.6     70.10    10.6     38.5   \n",
       "Louisiana             3806    3545         2.8     68.76    13.2     42.2   \n",
       "Maine                 1058    3694         0.7     70.39     2.7     54.7   \n",
       "Maryland              4122    5299         0.9     70.22     8.5     52.3   \n",
       "Massachusetts         5814    4755         1.1     71.83     3.3     58.5   \n",
       "Michigan              9111    4751         0.9     70.63    11.1     52.8   \n",
       "Minnesota             3921    4675         0.6     72.96     2.3     57.6   \n",
       "Mississippi           2341    3098         2.4     68.09    12.5     41.0   \n",
       "Missouri              4767    4254         0.8     70.69     9.3     48.8   \n",
       "Montana                746    4347         0.6     70.56     5.0     59.2   \n",
       "Nebraska              1544    4508         0.6     72.60     2.9     59.3   \n",
       "Nevada                 590    5149         0.5     69.03    11.5     65.2   \n",
       "New Hampshire          812    4281         0.7     71.23     3.3     57.6   \n",
       "New Jersey            7333    5237         1.1     70.93     5.2     52.5   \n",
       "New Mexico            1144    3601         2.2     70.32     9.7     55.2   \n",
       "New York             18076    4903         1.4     70.55    10.9     52.7   \n",
       "North Carolina        5441    3875         1.8     69.21    11.1     38.5   \n",
       "North Dakota           637    5087         0.8     72.78     1.4     50.3   \n",
       "Ohio                 10735    4561         0.8     70.82     7.4     53.2   \n",
       "Oklahoma              2715    3983         1.1     71.42     6.4     51.6   \n",
       "Oregon                2284    4660         0.6     72.13     4.2     60.0   \n",
       "Pennsylvania         11860    4449         1.0     70.43     6.1     50.2   \n",
       "Rhode Island           931    4558         1.3     71.90     2.4     46.4   \n",
       "South Carolina        2816    3635         2.3     67.96    11.6     37.8   \n",
       "South Dakota           681    4167         0.5     72.08     1.7     53.3   \n",
       "Tennessee             4173    3821         1.7     70.11    11.0     41.8   \n",
       "Texas                12237    4188         2.2     70.90    12.2     47.4   \n",
       "Utah                  1203    4022         0.6     72.90     4.5     67.3   \n",
       "Vermont                472    3907         0.6     71.64     5.5     57.1   \n",
       "Virginia              4981    4701         1.4     70.08     9.5     47.8   \n",
       "Washington            3559    4864         0.6     71.72     4.3     63.5   \n",
       "West Virginia         1799    3617         1.4     69.48     6.7     41.6   \n",
       "Wisconsin             4589    4468         0.7     72.48     3.0     54.5   \n",
       "Wyoming                376    4566         0.6     70.29     6.9     62.9   \n",
       "\n",
       "                Frost    Area  rain_mm  temp_celsius        sun_city  \\\n",
       "State                                                                  \n",
       "Alabama            20   50708     1480          17.1      Birmingham   \n",
       "Alaska            152  566432      572          -3.0       Anchorage   \n",
       "Arizona            15  113417      345          15.7          Tucson   \n",
       "Arkansas           65   51945     1284          15.8      Fort Smith   \n",
       "California         20  156361      563          15.2       San Diego   \n",
       "Colorado          166  103766      405           7.3  Grand Junction   \n",
       "Connecticut       139    4862     1279           9.4        Hartford   \n",
       "Delaware          103    1982     1160          12.9      Wilmington   \n",
       "Florida            11   54090     1385          21.5           Tampa   \n",
       "Georgia            60   58073     1287          17.5           Macon   \n",
       "Hawaii              0    6425     1618          21.1        Honolulu   \n",
       "Idaho             126   82677      481           6.9           Boise   \n",
       "Illinois          127   55748      996          11.0          Peoria   \n",
       "Indiana           122   36097     1060          10.9    Indianapolis   \n",
       "Iowa              140   55941      864           8.8      Des Moines   \n",
       "Kansas            114   81787      733          12.4         Wichita   \n",
       "Kentucky           95   39650     1242          13.1      Louisville   \n",
       "Louisiana          12   44930     1528          19.1     New Orleans   \n",
       "Maine             161   30920     1072           5.0        Portland   \n",
       "Maryland          101    9891     1131          12.3       Baltimore   \n",
       "Massachusetts     103    7826     1211           8.8          Boston   \n",
       "Michigan          125   56817      833           6.9         Lansing   \n",
       "Minnesota         160   79289      693           5.1     Minneapolis   \n",
       "Mississippi        50   47296     1499          17.4         Jackson   \n",
       "Missouri          108   68995     1071          12.5     Springfield   \n",
       "Montana           155  145587      390           5.9          Helena   \n",
       "Nebraska          139   76483      599           9.3         Lincoln   \n",
       "Nevada            188  109889      241           9.9            Reno   \n",
       "New Hampshire     174    9027     1103           6.6         Concord   \n",
       "New Jersey        115    7521     1196          11.5   Atlantic City   \n",
       "New Mexico        120  121412      370          11.9     Albuquerque   \n",
       "New York           82   47831     1062           7.4        Syracuse   \n",
       "North Carolina     80   48798     1279          15.0      Greensboro   \n",
       "North Dakota      186   69273      452           4.7        Bismarck   \n",
       "Ohio              124   40975      993          10.4        Columbus   \n",
       "Oklahoma           82   68782      927          15.3   Oklahoma City   \n",
       "Oregon             44   96184      695           9.1        Portland   \n",
       "Pennsylvania      126   44966     1089           9.3      Harrisburg   \n",
       "Rhode Island      127    1049     1218          10.1      Providence   \n",
       "South Carolina     65   30225     1264          16.9        Columbia   \n",
       "South Dakota      172   75955      511           7.3           Huron   \n",
       "Tennessee          70   41328     1376          14.2       Nashville   \n",
       "Texas              35  262134      734          18.2          Dallas   \n",
       "Utah              137   82096      310           9.2  Salt Lake City   \n",
       "Vermont           168    9267     1085           6.1      Burlington   \n",
       "Virginia           85   39780     1125          12.8        Richmond   \n",
       "Washington         32   66570      976           9.1         Seattle   \n",
       "West Virginia     100   24070     1147          11.0         Beckley   \n",
       "Wisconsin         149   54464      829           6.2         Madison   \n",
       "Wyoming           173   97203      328           5.6          Lander   \n",
       "\n",
       "                sun_perc  sun_hours  clear_days  \n",
       "State                                            \n",
       "Alabama             58.0     2641.0          99  \n",
       "Alaska              41.0     2061.0          61  \n",
       "Arizona             85.0     3806.0         193  \n",
       "Arkansas            61.0     2771.0         123  \n",
       "California          68.0     3055.0         146  \n",
       "Colorado            71.0     3204.0         136  \n",
       "Connecticut         56.0     2585.0          82  \n",
       "Delaware             NaN        NaN          97  \n",
       "Florida             66.0     2927.0         101  \n",
       "Georgia             66.0     2986.0         112  \n",
       "Hawaii              71.0        NaN          90  \n",
       "Idaho               64.0     2993.0         120  \n",
       "Illinois            56.0     2567.0          95  \n",
       "Indiana             55.0     2440.0          88  \n",
       "Iowa                59.0     2691.0         105  \n",
       "Kansas              65.0     2922.0         128  \n",
       "Kentucky            56.0     2514.0          93  \n",
       "Louisiana           57.0     2649.0         101  \n",
       "Maine               57.0     2513.0         101  \n",
       "Maryland            57.0     2582.0         105  \n",
       "Massachusetts       58.0     2634.0          98  \n",
       "Michigan            51.0     2392.0          71  \n",
       "Minnesota           58.0     2711.0          95  \n",
       "Mississippi         61.0     2720.0         111  \n",
       "Missouri            60.0     2690.0         115  \n",
       "Montana             59.0     2698.0          82  \n",
       "Nebraska            61.0     2762.0         117  \n",
       "Nevada              79.0     3646.0         158  \n",
       "New Hampshire       54.0     2519.0          90  \n",
       "New Jersey          56.0     2499.0          94  \n",
       "New Mexico          76.0     3415.0         167  \n",
       "New York            46.0     2120.0          63  \n",
       "North Carolina      60.0     2651.0         109  \n",
       "North Dakota        59.0     2738.0          93  \n",
       "Ohio                50.0     2183.0          72  \n",
       "Oklahoma            68.0     3089.0         139  \n",
       "Oregon              48.0     2341.0          68  \n",
       "Pennsylvania        58.0     2614.0          87  \n",
       "Rhode Island        58.0     2606.0          98  \n",
       "South Carolina      64.0     2826.0         115  \n",
       "South Dakota        63.0     2947.0         104  \n",
       "Tennessee           56.0     2510.0         102  \n",
       "Texas               61.0     2850.0         135  \n",
       "Utah                66.0     3029.0         125  \n",
       "Vermont             49.0     2295.0          58  \n",
       "Virginia            63.0     2829.0         100  \n",
       "Washington          47.0     2170.0          58  \n",
       "West Virginia        NaN        NaN          60  \n",
       "Wisconsin           54.0     2428.0          89  \n",
       "Wyoming             68.0     3073.0         114  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df = pd.read_csv(\"C:\\\\Users\\\\uxiat\\\\Desktop\\\\MasterBD\\\\Python\\\\weather_usa.csv\")\n",
    "weather_df.set_index('State', inplace=True)\n",
    "x77_weather_df = state_x77.merge(weather_df, left_index=True, right_on=\"State\")\n",
    "life_sun_corr = x77_weather_df['Life Exp'].corr(x77_weather_df['sun_hours'])\n",
    "north_south_regex = f\"(North|South).*\"\n",
    "north_south_mask = x77_weather_df.index.to_series().str.match(north_south_regex)\n",
    "north_south_df = x77_weather_df[north_south_mask]\n",
    "x77_weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0edb948b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5558fed7306490a01314279117fe2f3",
     "grade": true,
     "grade_id": "ej5-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(x77_weather_df, pd.core.frame.DataFrame) and\n",
    "       isinstance(north_south_df, pd.core.frame.DataFrame) and\n",
    "       isinstance(weather_df, pd.core.frame.DataFrame))\n",
    "assert(isinstance(north_south_mask, pd.core.frame.Series))\n",
    "assert(x77_weather_df.shape == (50, 14) and\n",
    "       north_south_df.shape == (4, 14))\n",
    "assert(\"|\" in north_south_regex and \n",
    "       \" \" not in north_south_regex)\n",
    "assert(north_south_df[\"Population\"].mean() == 2393.75)\n",
    "assert(round(north_south_df[\"temp_celsius\"].mean(), 3) == 10.975)\n",
    "assert(round(life_sun_corr, 3) == -0.043)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4358ff99c490dc81e0b845f54ed169c4f1577d3eca8e04c4eb34520c8284d25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
